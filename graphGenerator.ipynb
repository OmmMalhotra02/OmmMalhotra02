{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZI0XcPkvCmE4fQTOvDMJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmmMalhotra02/Project1/blob/main/graphGenerator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook 2: Graph Generation from Handwritten Diagrams\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import networkx as nx # graph studying\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 3: Define the VGGNet Model\n",
        "VGG_types = {\n",
        "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"VGG16\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
        "    \"VGG19\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"]\n",
        "}\n",
        "\n",
        "VGGType = \"VGG16\"\n",
        "\n",
        "class VGGnet(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=1000):\n",
        "        super(VGGnet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv_layers = self.create_conv_layers(VGG_types[VGGType])\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fcs(x)\n",
        "        return x\n",
        "\n",
        "    def create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == int:\n",
        "                out_channels = x\n",
        "\n",
        "                layers += [\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=in_channels,\n",
        "                        out_channels=out_channels,\n",
        "                        kernel_size=(3, 3),\n",
        "                        stride=(1, 1),\n",
        "                        padding=(1, 1),\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(x),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "                in_channels = x\n",
        "            elif x == \"M\":\n",
        "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "# Step 4: Preprocess Image\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV)\n",
        "    return binary_image\n",
        "\n",
        "# Step 5: Classify Image\n",
        "def classify_image(image_path, model, device):\n",
        "    transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Step 6: Extract Graph Elements\n",
        "def extract_graph_elements(image):\n",
        "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    nodes = []\n",
        "    edges = []\n",
        "\n",
        "    for contour in contours:\n",
        "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center = (int(x), int(y))\n",
        "        radius = int(radius)\n",
        "        if radius > 10:  # Adjust this threshold based on your dataset\n",
        "            nodes.append(center)\n",
        "        else:\n",
        "            edges.append(contour)\n",
        "\n",
        "    return nodes, edges\n",
        "\n",
        "# Step 7: Reconstruct and Display Graph\n",
        "def reconstruct_and_display_graph(nodes, edges):\n",
        "    G = nx.Graph()\n",
        "    for i, node in enumerate(nodes):\n",
        "        G.add_node(i, pos=node)\n",
        "\n",
        "    for edge in edges:\n",
        "        if len(edge) > 1:\n",
        "            (x1, y1), (x2, y2) = edge[0][0], edge[-1][0]\n",
        "            node1 = min(range(len(nodes)), key=lambda i: np.linalg.norm(np.array(nodes[i]) - np.array((x1, y1))))\n",
        "            node2 = min(range(len(nodes)), key=lambda i: np.linalg.norm(np.array(nodes[i]) - np.array((x2, y2))))\n",
        "            G.add_edge(node1, node2)\n",
        "\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "    nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=10, font_weight=\"bold\")\n",
        "    plt.show()\n",
        "\n",
        "# Step 8: Define Paths and Load Model\n",
        "image_path = '/content/drive/My Drive/dataset/test/some_image.png'  # Replace with your image path\n",
        "model_path = '/content/drive/My Drive/vgg_model.pth'  # Replace with your model path\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_classes = 5  # Replace with the number of classes in your dataset\n",
        "model = VGGnet(in_channels=3, num_classes=num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "# Step 9: Preprocess Image\n",
        "binary_image = preprocess_image(image_path)\n",
        "\n",
        "# Step 10: Classify Image\n",
        "class_index = classify_image(image_path, model, device)\n",
        "class_names = ['bellmanFord', 'dijkstra', 'useCase', 'sequence', 'activity']  # Replace with your class names\n",
        "print(f\"Classified as: {class_names[class_index]}\")\n",
        "\n",
        "# Step 11: Extract Graph Elements\n",
        "nodes, edges = extract_graph_elements(binary_image)\n",
        "\n",
        "# Step 12: Reconstruct and Display Graph\n",
        "reconstruct_and_display_graph(nodes, edges)\n",
        "\n"
      ],
      "metadata": {
        "id": "yD63t1GLQSAP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "210e30f1-18cd-4fb9-b3ec-77c4b32cd1f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Classified as: bellmanFord\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook 2: Graph Generation from Handwritten Diagrams\n",
        "\n",
        "# Step 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Step 2: Import Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 3: Define the VGGNet Model\n",
        "VGG_types = {\n",
        "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"VGG16\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
        "    \"VGG19\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"]\n",
        "}\n",
        "\n",
        "VGGType = \"VGG16\"\n",
        "\n",
        "class VGGnet(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=1000):\n",
        "        super(VGGnet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv_layers = self.create_conv_layers(VGG_types[VGGType])\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fcs(x)\n",
        "        return x\n",
        "\n",
        "    def create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == int:\n",
        "                out_channels = x\n",
        "\n",
        "                layers += [\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=in_channels,\n",
        "                        out_channels=out_channels,\n",
        "                        kernel_size=(3, 3),\n",
        "                        stride=(1, 1),\n",
        "                        padding=(1, 1),\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(x),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "                in_channels = x\n",
        "            elif x == \"M\":\n",
        "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "# Step 4: Preprocess Image\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV)\n",
        "    return binary_image\n",
        "\n",
        "# Step 5: Classify Image\n",
        "def classify_image(image_path, model, device):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "# Step 6: Extract Graph Elements\n",
        "def extract_graph_elements(image):\n",
        "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    nodes = []\n",
        "    edges = []\n",
        "\n",
        "    for contour in contours:\n",
        "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center = (int(x), int(y))\n",
        "        radius = int(radius)\n",
        "        if radius > 10:  # Adjust this threshold based on your dataset\n",
        "            nodes.append(center)\n",
        "        else:\n",
        "            edges.append(contour)\n",
        "\n",
        "    return nodes, edges\n",
        "\n",
        "# Step 7: Refine Edge Detection\n",
        "def refine_edges(edges, nodes):\n",
        "    refined_edges = []\n",
        "    for edge in edges:\n",
        "        if len(edge) > 1:\n",
        "            (x1, y1), (x2, y2) = edge[0][0], edge[-1][0]\n",
        "            # Find the closest nodes for the start and end points of the edge\n",
        "            node1 = min(range(len(nodes)), key=lambda i: np.linalg.norm(np.array(nodes[i]) - np.array((x1, y1))))\n",
        "            node2 = min(range(len(nodes)), key=lambda i: np.linalg.norm(np.array(nodes[i]) - np.array((x2, y2))))\n",
        "            if node1 != node2:\n",
        "                refined_edges.append((node1, node2))\n",
        "    return refined_edges\n",
        "\n",
        "# Step 8: Reconstruct and Display Graph\n",
        "def reconstruct_and_display_graph(nodes, edges):\n",
        "    G = nx.DiGraph()  # Directed graph\n",
        "    for i, node in enumerate(nodes):\n",
        "        G.add_node(i, pos=node)\n",
        "\n",
        "    refined_edges = refine_edges(edges, nodes)\n",
        "    for edge in refined_edges:\n",
        "        G.add_edge(*edge)\n",
        "\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "    nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", arrows=True)\n",
        "    plt.show()\n",
        "\n",
        "# Step 9: Define Paths and Load Model\n",
        "image_path = '/content/drive/My Drive/dataset/test/some_image.png'  # Replace with your image path\n",
        "model_path = '/content/drive/My Drive/vgg_model.pth'  # Replace with your model path\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_classes = 5  # Replace with the number of classes in your dataset\n",
        "model = VGGnet(in_channels=3, num_classes=num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "# Step 10: Preprocess Image\n",
        "binary_image = preprocess_image(image_path)\n",
        "\n",
        "# Step 11: Classify Image\n",
        "class_index = classify_image(image_path, model, device)\n",
        "class_names = ['bellmanFord', 'dijkstra', 'useCase', 'sequence', 'activity']  # Replace with your class names\n",
        "print(f\"Classified as: {class_names[class_index]}\")\n",
        "\n",
        "# Step 12: Extract Graph Elements\n",
        "nodes, edges = extract_graph_elements(binary_image)\n",
        "\n",
        "# Step 13: Reconstruct and Display Graph\n",
        "reconstruct_and_display_graph(nodes, edges)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "6iYA9s5KMtD5",
        "outputId": "4590ef60-a93a-4b63-a3d0-93857de85bf5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Classified as: bellmanFord\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRqfE2QPK2yd",
        "outputId": "5627fdd6-ba1a-48c8-bad9-d88db8a81548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "ww4kL9MsK7Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VGG_types = {\n",
        "    \"VGG11\": [64, \"M\", 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"VGG13\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, \"M\", 512, 512, \"M\", 512, 512, \"M\"],\n",
        "    \"VGG16\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, \"M\", 512, 512, 512, \"M\", 512, 512, 512, \"M\"],\n",
        "    \"VGG19\": [64, 64, \"M\", 128, 128, \"M\", 256, 256, 256, 256, \"M\", 512, 512, 512, 512, \"M\", 512, 512, 512, 512, \"M\"]\n",
        "}\n",
        "\n",
        "VGGType = \"VGG16\"\n",
        "\n",
        "class VGGnet(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_classes=1000):\n",
        "        super(VGGnet, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.conv_layers = self.create_conv_layers(VGG_types[VGGType])\n",
        "        self.fcs = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fcs(x)\n",
        "        return x\n",
        "\n",
        "    def create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == int:\n",
        "                out_channels = x\n",
        "\n",
        "                layers += [\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=in_channels,\n",
        "                        out_channels=out_channels,\n",
        "                        kernel_size=(3, 3),\n",
        "                        stride=(1, 1),\n",
        "                        padding=(1, 1),\n",
        "                    ),\n",
        "                    nn.BatchNorm2d(x),\n",
        "                    nn.ReLU(),\n",
        "                ]\n",
        "                in_channels = x\n",
        "            elif x == \"M\":\n",
        "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
        "\n",
        "        return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "p1UHNXUYK_aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV)\n",
        "    return binary_image\n",
        "\n",
        "def classify_image(image_path, model, device):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "def extract_graph_elements(image):\n",
        "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    nodes = []\n",
        "    edges = []\n",
        "\n",
        "    for contour in contours:\n",
        "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center = (int(x), int(y))\n",
        "        radius = int(radius)\n",
        "        if radius > 10:  # Adjust this threshold based on your dataset\n",
        "            nodes.append(center)\n",
        "        else:\n",
        "            edges.append(contour)\n",
        "\n",
        "    return nodes, edges\n",
        "\n",
        "def reconstruct_and_display_graph(nodes, edges):\n",
        "    G = nx.Graph()\n",
        "    for i, node in enumerate(nodes):\n",
        "        G.add_node(i, pos=node)\n",
        "\n",
        "    for edge in edges:\n",
        "        (x1, y1), (x2, y2) = edge[0][0], edge[1][0]\n",
        "        node1 = min(range(len(nodes)), key=lambda i: np.linalg.norm(np.array(nodes[i]) - np.array((x1, y1))))\n",
        "        node2 = min(range(len(nodes)), key=lambda i: np.linalg.norm(np.array(nodes[i]) - np.array((x2, y2))))\n",
        "        G.add_edge(node1, node2)\n",
        "\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "    nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=10, font_weight=\"bold\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "IZpQ10fRLBMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KixnjRoHJqs8"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    _, binary_image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV)\n",
        "    return binary_image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "def classify_image(image_path, model, device):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "    return predicted.item()\n"
      ],
      "metadata": {
        "id": "pXXJ6m6zKJkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_graph_elements(image):\n",
        "    contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    nodes = []\n",
        "    edges = []\n",
        "\n",
        "    for contour in contours:\n",
        "        (x, y), radius = cv2.minEnclosingCircle(contour)\n",
        "        center = (int(x), int(y))\n",
        "        radius = int(radius)\n",
        "        if radius > 10:  # Adjust this threshold based on your dataset\n",
        "            nodes.append(center)\n",
        "        else:\n",
        "            edges.append(contour)\n",
        "\n",
        "    return nodes, edges\n"
      ],
      "metadata": {
        "id": "UEc74kpbKPvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reconstruct_and_display_graph(nodes, edges):\n",
        "    G = nx.Graph()\n",
        "    for i, node in enumerate(nodes):\n",
        "        G.add_node(i, pos=node)\n",
        "\n",
        "    for edge in edges:\n",
        "        (x1, y1), (x2, y2) = edge[0][0], edge[1][0]\n",
        "        node1 = min(range(len(nodes)), key=lambda i: np.linalg.norm(np.array(nodes[i]) - np.array((x1, y1))))\n",
        "        node2 = min(range(len(nodes)), key=lambda i: np.linalg.norm(np.array(nodes[i]) - np.array((x2, y2))))\n",
        "        G.add_edge(node1, node2)\n",
        "\n",
        "    pos = nx.get_node_attributes(G, 'pos')\n",
        "    nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=10, font_weight=\"bold\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "hHjoEk4lKVd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths and model\n",
        "image_path = '/content/drive/My Drive/dataset/test/some_image.png'  # Replace with your image path\n",
        "model_path = '/content/drive/My Drive/vgg_model.pth'  # Replace with your model path\n",
        "\n",
        "# Load model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "num_classes = 5  # Replace with the number of classes in your dataset\n",
        "model = VGGnet(in_channels=3, num_classes=num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "# Preprocess image\n",
        "binary_image = preprocess_image(image_path)\n",
        "\n",
        "# Classify image\n",
        "class_index = classify_image(image_path, model, device)\n",
        "class_names = ['bellmanFord', 'dijkstra', 'useCase', 'sequence', 'activity']  # Replace with your class names\n",
        "print(f\"Classified as: {class_names[class_index]}\")\n",
        "\n",
        "# Extract graph elements\n",
        "nodes, edges = extract_graph_elements(binary_image)\n",
        "\n",
        "# Reconstruct and display graph\n",
        "reconstruct_and_display_graph(nodes, edges)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "TnjFQdG_KZVG",
        "outputId": "616f1062-57b3-46c1-bdf8-10b0fa87e4c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/vgg_model.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-13b8ff44baf5>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m  \u001b[0;31m# Replace with the number of classes in your dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGGnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Preprocess image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/vgg_model.pth'"
          ]
        }
      ]
    }
  ]
}